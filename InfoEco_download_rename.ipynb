{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from bs4 import SoupStrainer\n",
    "from selenium import webdriver\n",
    "from contextlib import contextmanager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from time import sleep\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "создадим папку с названием `download`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DOWNLOAD = 'download'\n",
    "if not os.path.exists(PATH_TO_DOWNLOAD):\n",
    "    os.mkdir(PATH_TO_DOWNLOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "    'Accept-Language': 'en-US,en;q=0.8',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "prefs = {\n",
    "    \"download.default_directory\": f'{os.getcwd() + \"/\" + PATH_TO_DOWNLOAD + \"/\"}',\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "}\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
    "\n",
    "SITE = \"http://www.infoeco.ru/\"\n",
    "\n",
    "# переходим на страницу 2017 года\n",
    "def get_front():\n",
    "    driver.get(f\"{SITE}index.php?id=3122\")\n",
    "    html = driver.page_source\n",
    "    soup = BS(html, 'html.parser')\n",
    "    divs = soup.find_all(\"ul\", class_=\"nav\")[1].find_all(\"li\")\n",
    "    pat = re.compile(r\"\\d{4}\")\n",
    "    needed_hrefs = [d.a['href'] for d in divs if pat.search(d.a['title'])]\n",
    "    driver.quit()\n",
    "    if not needed_hrefs:\n",
    "        get_front()\n",
    "    return needed_hrefs\n",
    "    \n",
    "needed_hrefs = get_front()\n",
    "needed_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_year_content(idx=1, href=\"/index.php?id=3122\"):\n",
    "    '''\n",
    "    input: \n",
    "        idx = year - 2016\n",
    "        href = main link for 2017 eco data    \n",
    "    '''\n",
    "    driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
    "    driver.get(f\"{SITE}{href}\")\n",
    "    html = driver.page_source\n",
    "    soup = BS(html, 'html.parser')\n",
    "    divs = soup.find_all(\"ul\")[1].find_all(\"li\")\n",
    "    div = divs[idx] # здесь хранятся данные по годам\n",
    "    \n",
    "    for i in div.find_all(\"a\")[1:13]: # первый тег - год, нужны месяцы январь - декабрь\n",
    "        driver.get(f\"{SITE}{i['href']}\")\n",
    "        href = i['href']\n",
    "    \n",
    "        try:\n",
    "            sleep(1)\n",
    "            driver.find_element_by_xpath(f\"//a[@href='{href}']\").click()\n",
    "            sleep(1)\n",
    "        except NoSuchElementException:\n",
    "            print(href, 'rip')\n",
    "            continue\n",
    "\n",
    "        html = driver.page_source\n",
    "        only_section = SoupStrainer(\"section\")\n",
    "        soup = BS(html, 'html.parser', parse_only=only_section)\n",
    "        snd_page = soup.find(\"div\", class_=\"pager\").a['href'] # вторая страница\n",
    "        \n",
    "        def loader(second_page=False):\n",
    "            if second_page:\n",
    "                try:\n",
    "                    driver.get(f\"{SITE}{snd_page}\")\n",
    "                    sleep(1)\n",
    "                except NoSuchElementException:\n",
    "                    print(\"there's no second page\")\n",
    "                    return\n",
    "            html = driver.page_source\n",
    "            soup = BS(html, 'html.parser', parse_only=only_section)\n",
    "            divs = soup.find_all('div')\n",
    "            for div in divs:\n",
    "        \n",
    "                href = div.a['href']\n",
    "                try:\n",
    "                    session = requests.Session()\n",
    "                    if session.get(f\"{SITE}{href}\", headers=headers).status_code != 200:\n",
    "                        print(f\"{href} is broken! {date}\")\n",
    "                        continue\n",
    "                    sleep(1)\n",
    "                    driver.find_element_by_xpath(f\"//a[@href='{href}']\").click()\n",
    "                    sleep(1)\n",
    "                except:\n",
    "                    print(f\"{href} is stuck here ({date})\")\n",
    "                    continue\n",
    "\n",
    "        loader()\n",
    "        loader(second_page=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Описание\n",
    "следующие 4 ячейки будут скачивать файлы для 2017 - н.в.\n",
    "\n",
    "в 2016 у них очень странная структра (в одном файле записаны данные сразу за несколько дней, поэтому я решил не включать его в парсинг\n",
    "\n",
    "первый аргумент - это индекс, он отвечает за год, e.g. `1 == 2017`, `2 == 2018` ...\n",
    "\n",
    "в принте должны выдаваться сообщения о том, за какие числа браузер не смог скачать файлы\n",
    "если ячейка закончила работу очень быстро - запуск еще раз\n",
    "\n",
    "P.S. файлы скачиваются довольно долго (полчаса - час)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_year_content(1, needed_hrefs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_year_content(2, needed_hrefs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "download_year_content(3, needed_hrefs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "download_year_content(4, needed_hrefs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "заплатка для названия файлов\n",
    "\n",
    "формат: `ddmmyyyy.docx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_names = [] #  новые соотв. названия файлов\n",
    "\n",
    "@contextmanager\n",
    "def rename_docx_files(dest, new_names):\n",
    "    try:\n",
    "        cwd = os.getcwd()\n",
    "        os.chdir(dest)\n",
    "        files = sorted(os.listdir(), key=os.path.getmtime)\n",
    "        start = re.compile(r'^201\\d')\n",
    "        end = re.compile(r'201\\d$')\n",
    "        for i in files:\n",
    "            try:\n",
    "                x = re.search('\\d[^a-zA-Z]*\\d', i).group(0)\n",
    "                if '-' in x or '_' in x or '.' in x:\n",
    "                    x = re.sub(r'[._-]', '', x)\n",
    "                if '00' in x and len(x) > 8:\n",
    "                    x = re.sub('00', '0', x)\n",
    "                if re.search(start, x) and not re.search(end, x):\n",
    "                    'reversed date'\n",
    "                    year = x[:4]\n",
    "                    month = x[4:6]\n",
    "                    day = x[6:8]\n",
    "                    x = day + month + year\n",
    "                if len(x) < 8:\n",
    "                    x = x[:-2] + '20' + x[-2:]\n",
    "                if x == \"0328032018\":\n",
    "                    x = \"03282018\"\n",
    "                if len(x) > 8:\n",
    "                    x = x[:8]\n",
    "                date_names.append(x)\n",
    "        except:\n",
    "            print('WTF', i) #  не должен триггериться (имхо)\n",
    "        \n",
    "        # переименовываем\n",
    "        for i in range(len(files)):\n",
    "            try:\n",
    "                os.rename(files[i], new_names[i] + \".docx\")                \n",
    "            except FileNotFoundError:\n",
    "                print(f\"no such file {files[i]}\")\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rename_docx_files(PATH_TO_DOWNLOAD, date_names):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls download | wc -l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
